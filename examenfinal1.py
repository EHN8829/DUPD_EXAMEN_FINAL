# -*- coding: utf-8 -*-
"""ExamenFinal1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15E1_ZISjXVhZLlpk_PRYchYrtt9lxa9S

--------------------------------------------------------------------------------

$$\small\textbf{Análisis de sensaciones a videos de Ecuaciones Diferenciales en YouTube como material de estudio complementario}$$

--------------------------------------------------------------------------------

$\small\text{Autor: Eginhardo Navarro Honda}$

$$\small\textbf{Examen final}$$

$\large\text{1. Descarga de comentarios de YouTube}$
"""

import googleapiclient.discovery
import pandas as pd

# API KEY
api_key = 'AIzaSyB0laR8XCbWObyESyPJ0zU0V4IAy9HdwHE'

# Configuración del cliente YouTube
youtube = googleapiclient.discovery.build("youtube", "v3", developerKey=api_key)

def get_video_ids_from_playlist(playlist_id):
    video_ids = []
    request = youtube.playlistItems().list(
        part="contentDetails",
        playlistId=playlist_id,
        maxResults=50
    )
    response = request.execute()

    while request is not None:
        for item in response['items']:
            video_ids.append(item['contentDetails']['videoId'])
        if 'nextPageToken' in response:
            request = youtube.playlistItems().list(
                part="contentDetails",
                playlistId=playlist_id,
                pageToken=response['nextPageToken'],
                maxResults=50
            )
            response = request.execute()
        else:
            break
    return video_ids

# Playlist ID (agregando el valor del playlist)
playlist_id = 'PLeySRPnY35dFSDPi_4Q5R1VCGL_pab26A'

# Obteniendo los IDs de los videos
video_ids = get_video_ids_from_playlist(playlist_id)

# Descargando comentarios de cada video
all_comments = []
for video_id in video_ids:
    comments = get_comments(video_id)  # Función 'get_comments' que ya se tiene
    all_comments.extend(comments)

# Creando un DataFrame con todos los comentarios (texto)
df = pd.DataFrame(all_comments, columns=['text'])
print(df.head())

df.head(25)

"""$\small\text{1.1. Preprocesamiento de Datos}$"""

import re
from nltk.corpus import stopwords

# Eliminando caracteres especiales y convertir a minúsculas
def clean_text(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Eliminar caracteres especiales
    text = text.lower()  # Convirtiendo a minúsculas
    return text
# Aplicando la limpieza al DataFrame
df['clean_text'] = df['text'].apply(clean_text)

# Listando manual de stopwords en español
stop_words = set([
    'yo', 'me', 'mi', 'mío', 'mía', 'míos', 'mías', 'nosotros', 'nosotras', 'nuestro', 'nuestra', 'nuestros', 'nuestras',
    'tú', 'te', 'ti', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'vosotros', 'vosotras', 'vuestro', 'vuestra', 'vuestros', 'vuestras',
    'él', 'ella', 'ello', 'lo', 'la', 'los', 'las', 'le', 'les', 'se', 'sí', 'suyo', 'suya', 'suyos', 'suyas', 'ellos', 'ellas',
    'uno', 'una', 'unos', 'unas', 'algo', 'alguien', 'nadie', 'quien', 'quienes', 'cual', 'cuales', 'cuyo', 'cuya', 'cuyos', 'cuyas',
    'que', 'qué', 'como', 'cómo', 'cuando', 'cuándo', 'donde', 'dónde', 'por', 'para', 'con', 'sin', 'sobre', 'tras', 'entre',
    'de', 'del', 'al', 'a', 'en', 'por', 'ante', 'bajo', 'cabe', 'contra', 'desde', 'durante', 'hacia', 'hasta', 'según', 'sin',
    'so', 'sobre', 'tras', 'y', 'o', 'u', 'pero', 'sino', 'porque', 'pues', 'aunque', 'también', 'además', 'antes', 'después',
    'luego', 'entonces', 'mientras', 'así', 'tan', 'tanto', 'muy', 'ya', 'aún', 'aun', 'más', 'menos', 'nunca', 'siempre',
    'también', 'tampoco', 'cada', 'todo', 'todos', 'toda', 'todas', 'algún', 'ningún', 'ninguna', 'ninguno', 'otra', 'otro',
    'otros', 'otras', 'poco', 'poca', 'pocos', 'pocas', 'mucho', 'mucha', 'muchos', 'muchas', 'varios', 'varias', 'sí', 'no',
    'nada', 'todo', 'esto', 'eso', 'aquello', 'aquí', 'ahí', 'allí', 'allá', 'acá', 'ahora', 'ayer', 'hoy', 'mañana', 'sí', 'donde',
    'qué', 'quién', 'cómo', 'cuánto', 'etc'
])

# Eliminando stopwords
df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

# Creando un DataFrame con todos los comentarios (tabla)
df.head(25)

"""$\large\text{2. Análisis Estadístico (Cualitativo y Cuantitativo)}$

$\small\text{2.1. Cualitativo}$
"""

# Contando palabras
df['word_count'] = df['text'].apply(lambda x: len(x.split()))

# Distribución de la longitud de los comentarios
print(df['word_count'].describe())

"""$\small\text{2.2. Cuantitativo}$

Análisis de sensaciones con 'TextBlob'
"""

from textblob import TextBlob

# Calculando la polaridad
df['polarity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Observando la distribución de la polaridad (texto)
print(df['polarity'].describe())

# Observando la distribución de la polaridad (tabla)
df['polarity'].describe()

"""$\large\text{3. Polaridad en intervalos (desigualdades)}$"""

#from textblob import TextBlob

# Función para analizar la polaridad del sentimiento
#def analizar_sentimiento(texto):
    #analisis = TextBlob(texto)
    #return analisis.sentiment.polarity

from textblob import TextBlob

# Calculando la polaridad
df['polarity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Clasificando la polaridad en intervalos utilizando desigualdades
def classify_polarity(polarity):
    if polarity < -0.5:
        return 'Polarity < -0.5'
    elif -0.5 <= polarity < 0:
        return '-0.5 <= Polarity < 0'
    elif 0 <= polarity < 0.5:
        return '0 <= Polarity < 0.5'
    else:
        return 'Polarity >= 0.5'

df['polarity_interval'] = df['polarity'].apply(classify_polarity)

# Observando la distribución de la polaridad con los intervalos (texto)
print(df['polarity_interval'].value_counts())

# Observando la distribución de la polaridad con los intervalos (tabla)
df['polarity_interval'].value_counts()

"""$\large\text{4. Visualización de Polaridad}$"""

import seaborn as sns
import matplotlib.pyplot as plt

# Histograma de la polaridad
sns.histplot(df['polarity'], kde=True)
plt.title('Distribución de Polaridad en Comentarios de YouTube')
plt.xlabel('Polaridad')
plt.ylabel('Frecuencia')
plt.show()

"""$\large\text{5. Análisis Inferencial (T-test)}$

$\small\text{Comparación de polaridades entre categorías positivas y negativas, usando un análisis de hipótesis}$
"""

# Categorizando comentarios en positivos y negativos
positive_comments = df[df['polarity'] > 0]
negative_comments = df[df['polarity'] < 0]

# Prueba t entre ambos grupos
from scipy.stats import ttest_ind

t_stat, p_value = ttest_ind(positive_comments['polarity'], negative_comments['polarity'])
print(f'T-statistic: {t_stat}, P-value: {p_value}')

"""$\large\text{6. Guardado y Subida el Modelo a Streamlit}$

$\small\text{6.1. Guardado del modelo}$
"""

import pickle

# Guardar DataFrame de análisis
df.to_csv('youtube_comments_analysis.csv', index=False)

"""$\small\text{6.2. Descarga del archivo [youtube_comments_analysis.csv]}$"""

from google.colab import files

# Descargando el archivo CSV
files.download('youtube_comments_analysis.csv')

"""$\small\text{6.3. Para Streamlit Cloud (Opcional)}$"""

#!pip install streamlit

import streamlit as st
import pandas as pd

# Cargando los resultados
df = pd.read_csv('youtube_comments_analysis.csv')

# Mostrando polaridad
st.write(df.head())

# Mostrando gráfico
import seaborn as sns
import matplotlib.pyplot as plt

sns.histplot(df['polarity'], kde=True)
plt.title('Distribución de Polaridad en Comentarios de YouTube')
plt.xlabel('Polaridad')
plt.ylabel('Frecuencia')
st.pyplot(plt)

"""$\large\text{7. Entrenamiento del Modelo (Naive Bayes)}$

$\text{Se han deshabilitado las librerías descritas por ahora. Se importarán más adelante}$
"""

#from sklearn.model_selection import train_test_split
#from sklearn.feature_extraction.text import CountVectorizer
#from sklearn.naive_bayes import MultinomialNB

"""$\small\text{Pasos para Corrección del Código}$

$\small\text{Paso 1: Verificando si hay valores NaN en df['clean_text']}$

$\small\text{Comprobando la existencia de valores nulos antes de aplicar [CountVectorizer]}$
"""

# Verificando valores NaN en 'clean_text'
nan_count = df['clean_text'].isnull().sum()
print(f'Cantidad de valores NaN en clean_text: {nan_count}')

"""$\small\text{Si [nan_count>0] se necesita manejar estos valores}$

$\small\text{Paso 2: Manejando los valores NaN}$

$\small\text{Se tiene dos opciones}$

1.   $\small\text{Opción A: Eliminar las filas con valores NaN}$
2.   $\small\text{Opción B: Reemplazar los valores NaN por una cadena vacía}$

$\small\text{Opción A: Eliminar filas con NaN}$
"""

# Eliminando filas con NaN en 'clean_text'
df = df.dropna(subset=['clean_text'])

"""$\small\text{Para este caso, la opción A es preferible porque evita procesar datos que podrían no aportar información}$

$\small\text{Opción B: Reemplazando NaN con cadena vacía}$
"""

# Reemplazando NaN con cadena vacía
df['clean_text'] = df['clean_text'].fillna('')

"""$\small\text{Paso 3: Asegurando que las etiquetas coincidan con los datos}$

$\small\text{Después de eliminar o modificar las filas, el tamaño del DataFrame df puede cambiar}$
$\small\text{Por lo tanto, necesitamos asegurarnos de que las etiquetas (df['label']) tengan la misma longitud que los datos de entrada}$
"""

# Recalculando las etiquetas después de manejar NaN
df.reset_index(drop=True, inplace=True)
df['label'] = [1 if i % 2 == 0 else 0 for i in range(len(df))]

"""$\small\text{Paso 4: Vectorizando el texto}$

$\small\text{Entonces, aplicando [CountVectorizer] sin problemas}$
"""

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['clean_text'])

"""$\small\text{Paso 5: Dividiendo los datos}$"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.2)

"""$\small\text{Paso 6: Entrenando el modelo}$"""

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(X_train, y_train)

"""$\small\text{Paso 7: Evaluando el modelo}$"""

accuracy = model.score(X_test, y_test)
print(f'Precisión del modelo: {accuracy}')

"""$\small\textbf{Código completo corregido}$"""

# Paso 1: Verificando valores NaN
nan_count = df['clean_text'].isnull().sum()
print(f'Cantidad de valores NaN en clean_text: {nan_count}')

# Paso 2: Manejando valores NaN
df = df.dropna(subset=['clean_text'])
# O, si prefieres:
# df['clean_text'] = df['clean_text'].fillna('')

# Paso 3: Asegurando que las etiquetas coincidan
df.reset_index(drop=True, inplace=True)
df['label'] = [1 if i % 2 == 0 else 0 for i in range(len(df))]

# Paso 4: Vectorizando el texto
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['clean_text'])

# Paso 5: Dividiendo los datos
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.2)

# Paso 6: Entrenando el modelo
from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(X_train, y_train)

# Paso 7: Evaluando el modelo
accuracy = model.score(X_test, y_test)
print(f'Precisión del modelo: {accuracy}')

"""$\small\textbf{Código resumido corregido}$"""

# Convirtiendo texto a matrices de conteo de palabras
vectorizer = CountVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['clean_text'])

# Etiquetas simuladas (positivo/negativo)
df['label'] = [1 if i % 2 == 0 else 0 for i in range(len(df))]  # Ejemplo de etiquetas

# Dividiendo datos
X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.2)

# Entrenando el modelo
model = MultinomialNB()
model.fit(X_train, y_train)

# Evaluando el modelo
accuracy = model.score(X_test, y_test)
print(f'Precisión del modelo: {accuracy}')

"""$\large\text{8. Análisis de sensaciones}$"""

from textblob import TextBlob
import nltk
nltk.download('punkt')

def analyze_sentiment(comments):
    sentiments = []
    for comment in comments:
        blob = TextBlob(comment)
        sentiments.append(blob.sentiment.polarity)  # Polaridad: De -1 (negativo) a 1 (positivo)
    return sentiments

"""$\small\text{8.1. Verificación del contenido en [video_sentiments]}$"""

import requests

# Definiendo la API_KEY de la API de YouTube
API_KEY = 'AIzaSyCn7cuWNo3HVAfXq2L3mAIkrXDu4n_tedk'  # Colocando la API key de la API de YouTube

def get_videos_from_playlist(playlist_id):
    url = f'https://www.googleapis.com/youtube/v3/playlistItems?part=contentDetails&maxResults=50&playlistId={playlist_id}&key={API_KEY}'
    response = requests.get(url).json()

    if 'items' not in response:
        print(f"Error: No se pudieron obtener los videos. Respuesta de la API: {response}")
        return []

    videos = [item['contentDetails']['videoId'] for item in response['items']]
    return videos

# Función para obtener los comentarios de un video
def get_video_comments(video_id):
    comments = []
    url = f'https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId={video_id}&key={API_KEY}&maxResults=100'
    response = requests.get(url).json()

    if 'items' not in response:
        print(f"Error: No se pudieron obtener los comentarios para el video {video_id}. Respuesta de la API: {response}")
        return comments

    for item in response['items']:
        comment = item['snippet']['topLevelComment']['snippet']['textOriginal']
        comments.append(comment)

    return comments

# Función para calcular el promedio de las sensaciones
def calculate_average_sentiment(sentiments):
    if sentiments:
        return sum(sentiments) / len(sentiments)
    return 0

# Función para analizar las sensaciones de los comentarios de todos los videos en la playlist
def analyze_playlist_sentiments(playlist_id):
    video_sentiments = {}
    video_ids = get_videos_from_playlist(playlist_id)

    if not video_ids:
        print("No se encontraron videos en la playlist o hubo un error.")
        return video_sentiments

    for video_id in video_ids:
        comments = get_video_comments(video_id)
        if comments:
            # Supongamos que analyze_sentiment devuelve una lista de puntajes de sentimientos
            sentiments = analyze_sentiment(comments)
            # Calculando el promedio de las sensaciones para este video
            avg_sentiment = calculate_average_sentiment(sentiments)
            video_sentiments[video_id] = avg_sentiment
        else:
            print(f"No se encontraron comentarios para el video {video_id}.")

    return video_sentiments

# Función para analizar las sensaciones de los comentarios de todos los videos en la playlist
def analyze_playlist_sentiments(playlist_id):
    video_sentiments = {}
    video_ids = get_videos_from_playlist(playlist_id)

    for video_id in video_ids:
        comments = get_video_comments(video_id)
        # Suponiendo que se dispone una función 'analyze_sentiment' definida previamente
        video_sentiments[video_id] = analyze_sentiment(comments)

    return video_sentiments

# ID de la playlist
playlist_id = 'PLeySRPnY35dFSDPi_4Q5R1VCGL_pab26A'

# Ejecutando la función para obtener los promedios de sensaciones de todos los videos en la playlist
video_sentiments = analyze_playlist_sentiments(playlist_id)

# Imprimiendo el promedio de las sensaciones para cada video
for video_id, avg_sentiment in video_sentiments.items():
    print(f'Video ID: {video_id}, Promedio de sentimientos: {avg_sentiment}')

"""El problema que se obnserva es la salida de la función 'analyze_sentiment' (comments) que está devolviendo una lista de polaridades para cada comentario en lugar de un promedio de los sentimientos.

Por eso, en lugar de un solo valor promedio, se obtiene una lista completa de valores de polaridad.

Para solucionarlo, debemos asegurarnos que se debe obtener el promedio de las sensaciones de cada video.

La modificación del código para calcular el promedio correcto se presenta a continuación:

1.   **Modificación en la función 'analyze_playlist_sentiments'**: En lugar de asignar la lista de sentimientos directamente al diccionario 'video_sentiments', se calcula el promedio de la lista de polaridades de cada video.
2.   **Cambio del formato de salida**: Para que devuelva el valor promedio para cada video en lugar de una lista completa.
"""

from textblob import TextBlob
import nltk
import requests

nltk.download('punkt')

# Función para analizar las sensaciones de un comentario
def analyze_sentiment(comments):
    sentiments = []
    for comment in comments:
        blob = TextBlob(comment)
        sentiments.append(blob.sentiment.polarity)  # Polaridad: De -1 (negativo) a 1 (positivo)
    return sentiments

# Función para obtener videos de una playlist
def get_videos_from_playlist(playlist_id):
    url = f'https://www.googleapis.com/youtube/v3/playlistItems?part=contentDetails&maxResults=50&playlistId={playlist_id}&key={API_KEY}'
    response = requests.get(url).json()

    if 'items' not in response:
        print(f"Error: No se pudieron obtener los videos. Respuesta de la API: {response}")
        return []

    return [item['contentDetails']['videoId'] for item in response['items']]

# Función para obtener comentarios de un video
def get_video_comments(video_id):
    comments = []
    url = f'https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId={video_id}&key={API_KEY}&maxResults=100'
    response = requests.get(url).json()

    if 'items' not in response:
        print(f"Error: No se pudieron obtener los comentarios para el video {video_id}. Respuesta de la API: {response}")
        return comments

    for item in response['items']:
        comment = item['snippet']['topLevelComment']['snippet']['textOriginal']
        comments.append(comment)

    return comments

# Función para calcular el promedio de las sensaciones
def calculate_average_sentiment(sentiments):
    if sentiments:
        return sum(sentiments) / len(sentiments)
    return 0

# Función para analizar las sensaciones de los comentarios de todos los videos en la playlist
def analyze_playlist_sentiments(playlist_id):
    video_sentiments = {}
    video_ids = get_videos_from_playlist(playlist_id)

    if not video_ids:
        print("No se encontraron videos en la playlist o hubo un error.")
        return video_sentiments

    for video_id in video_ids:
        comments = get_video_comments(video_id)
        if comments:
            sentiments = analyze_sentiment(comments)
            avg_sentiment = calculate_average_sentiment(sentiments)
            video_sentiments[video_id] = avg_sentiment
        else:
            print(f"No se encontraron comentarios para el video {video_id}.")

    return video_sentiments

# ID de la playlist
playlist_id = 'PLeySRPnY35dFSDPi_4Q5R1VCGL_pab26A'

# Ejecutando la función para obtener los promedios de sentimientos de todos los videos en la playlist
video_sentiments = analyze_playlist_sentiments(playlist_id)

# Imprimiendo el promedio de los sentimientos para cada video
for video_id, avg_sentiment in video_sentiments.items():
    print(f'Video_ID: {video_id}, Promedio_de_sentimientos: {avg_sentiment}')

import pandas as pd

# Datos (ejemplo) - ajuste de acuerdo a nuestras necesidades
data = {
    'Video_ID': ['rd2jKGQJucE', '_GW79tPHmVA', 'NYE1GhhiF7E', 'j1nwW8eyD0Q', '2S1XmMzFUe0', 'WAYBZTQoVkI', 'veSNESx8XBk', 'jA8nVFz94eY', 'A41Xtv_tatE', 'kufA4JGm_sU', '0pR_tZAVnUE', '1_kJsHwf64c', '2Rq34fbl1J4', 'Ksw0yqOK00I', 'v3smSegJR50', 'MyH8ItOpkjA', '-RHTuM3hNII', 'UVMtTO5H1bk', 'ejyLvEIpv-Q', 'yI1vvwPKB1c', 'h8JY9daqCBY', 'kqzOGLstWh8', 'SucuT1pw8Jc', 'QBrcu1kBd7k', 'lr9ToJnwPEk', 'E2g4UffqKw0'],
    'Promedio_de_sentimientos': [0.024, 0.025, 0.024, 0.057, 0.014, 0.039, -0.003, 0.021, 0.013, 0.058, 0.005, 0.025, 0.008, 0.013, 0.032, 0.010, -0.066, 0.019, 0.008, 0.006, 0.048, 0.045, 0.050, 0.023, 0.043, 0.065]
}

# Creando el DataFrame
df = pd.DataFrame(data)

# Calculando el promedio por video
df_avg = df.groupby('Video_ID').mean().reset_index()

# Guardando el DataFrame como CSV
df_avg.to_csv('promedio_por_video.csv', index=False)

print("CSV generado con éxito.")

# Mostrando el DataFrame
print(df)

df_avg.head(25)

"""$\small\text{8.2. Descarga de [promedio_por_video.csv] como archivo}$"""

from google.colab import files

# Activando una descarga en nuestro procesador
files.download('promedio_por_video.csv')

"""$\large\text{9. Gráficos}$"""

import matplotlib.pyplot as plt
import seaborn as sns

# Configurando el estilo de los gráficos
sns.set(style="whitegrid")

# Creando histograma
plt.figure(figsize=(10, 6))
sns.histplot(df['Promedio_de_sentimientos'], bins=20, kde=True, color='coral')
plt.title('Distribución de Sensaciones Promedio de Videos')
plt.xlabel('Sensación Promedio')
plt.ylabel('Frecuencia')
plt.show()

# Ordenando los valores para mejorar la visualización
df_sorted = df.sort_values(by='Promedio_de_sentimientos', ascending=False)

"""$\small\text{Verificación del DataFrame}$

$\text{Si no estamos seguros de cómo se llama la columna de los nombres de los videos, se verifica los nombres de las columnas con [print(df_sorted.columns)] o [df_sorted.columns]}$
"""

print(df_sorted.columns)

df_sorted.columns

import matplotlib.pyplot as plt
import seaborn as sns

# Suponiendo que tu DataFrame tiene una columna 'Video Name' con los nombres de los videos
plt.figure(figsize=(14, 8))

# Asegurando que 'Video Name' sea la columna en el eje 'x' y 'score' o la columna de promedio en el eje 'y'
sns.barplot(x='Video_ID', y='Promedio_de_sentimientos', data=df_sorted, palette='viridis')

plt.title('Sensaciones por Video')
plt.xlabel('Nombre del Video')
plt.ylabel('Sensación Promedio')

# Rotando las etiquetas del eje x
plt.xticks(rotation=90)

# Mostrando el gráfico
plt.show()

# Creando el gráfico de barras con una sintaxis ligeramente diferente
plt.figure(figsize=(14, 8))

# Línea de código diferente respecto al código arriba indicado
sns.barplot(x=df_sorted['Video_ID'], y=df_sorted['Promedio_de_sentimientos'], palette='viridis')

plt.title('Sensaciones por Video')
plt.xlabel('video')
plt.ylabel('Sensación Promedio')
plt.xticks(rotation=90)  # Rotando 90° las etiquetas del eje 'x' para mayor legibilidad
plt.show()

import pandas as pd
from textblob import TextBlob
import nltk
nltk.download('punkt')  # Descargando el tokenizador de nltk nuevamente

def analyze_sentiment(comment):
    blob = TextBlob(comment)
    return blob.sentiment.polarity  # Polaridad: -1 De (negativo) a 1 (positivo)

# Creando una lista para almacenar los comentarios positivos
positive_comments = []

for video_id, comments in video_comments.items():
    for comment in comments:
        sentiment = analyze_sentiment(comment)

        if isinstance(sentiment, list):  # Asegurando que sea una lista
            sentiment_score = sentiment[0]  # Asumiendo que el primer elemento es un valor numérico

        if sentiment_score > 0:  # Compararando con 0
            positive_comments.append({
                'Video ID': video_id,
                'Comment': comment,
                'Sentiment': sentiment_score
            })

def analyze_sentiment(comment):
    # Implementando un análisis de sensación
    # Asegurando la devolución a un solo valor numérico
    polarity = 0.5  # Ejemplo de un valor de polaridad
    return polarity

for video_id, comments in video_comments.items():
    for comment in comments:
        sentiment = analyze_sentiment(comment)

        if isinstance(sentiment, list) and len(sentiment) == 1:  # Si es una lista con un solo elemento
            sentiment_score = sentiment[0]

        if sentiment_score > 0:
            positive_comments.append({
                'Video ID': video_id,
                'Comment': comment,
                'Sentiment': sentiment_score
            })

# Analizando cada video y sus comentarios
for video_id, comments in video_comments.items():
    for comment in comments:
        sentiment = analyze_sentiment(comment)
        if sentiment > 0:  # Considerando como positivo si la polaridad es mayor que 0
            positive_comments.append({
                'Video ID': video_id,
                'Comment': comment,
                'Sentiment': sentiment
            })

# Convirtiendo la lista a un DataFrame
df_positive_comments = pd.DataFrame(positive_comments)

# Mostrando el DataFrame
print(df_positive_comments)

df_positive_comments

# Mostrando la tabla de comentarios positivos
plt.figure(figsize=(12, 8))
ax = sns.heatmap(df_positive_comments[['Comment', 'Sentiment']].head(20).set_index('Comment'), annot=True, cmap='YlGnBu')
ax.set_title('Comentarios Positivos')
plt.show()



import os
# Listar archivos en el directorio actual
print(os.listdir('.'))

import pandas as pd
from textblob import TextBlob
import seaborn as sns

# Importando el archivo de sensaciones desde GitHub
url = 'https://raw.githubusercontent.com/EHN8829/DUPD_FINAL2/main/youtube_comments.csv'
df = pd.read_csv(url)

# Mostrando las primeras filas del DataFrame
print(df.head())

df.head()

df['polaridad']=df['Comment'].apply(lambda x: TextBlob(x).sentiment.polarity)
df['subjetividad']=df['Comment'].apply(lambda x: TextBlob(x).sentiment.subjectivity)

import matplotlib.pyplot as plt
import seaborn as sns

# Distribución de la polaridad
sns.displot(df['polaridad'], color='navy')
plt.title('Distribución de la Polaridad')
plt.xlabel('Polaridad')
plt.ylabel('Frecuencia')
plt.show()

# Distribución de la subjetividad
sns.displot(df['subjetividad'], color='lightblue')
plt.title('Distribución de la Subjetividad')
plt.xlabel('Subjetividad')
plt.ylabel('Frecuencia')
plt.show()

#import seaborn as sns
#import matplotlib.pyplot as plt
#import pandas as pd

# Creando un gráfico superpuesto
plt.figure(figsize=(10, 6))  # Ajuste del tamaño del gráfico (según el caso)

# Graficando la polaridad en rojo oscuro
sns.histplot(df['polaridad'], color='crimson', kde=True, label='Polaridad')

# Graficando la subjetividad en verde claro
sns.histplot(df['subjetividad'], color='cornsilk', kde=True, label='Subjetividad')

# Ajustando las etiquetas y leyenda
plt.xlabel('Valor')
plt.ylabel('Frecuencia')
plt.title('Distribución de Polaridad y Subjetividad')
plt.legend()

# Mostrando el gráfico
plt.show()

"""1. Se observa que los valores para la subjetividad [cornsilk] y polaridad  
[crimson] son mayores a cero, respectivamente.

2. Significa que los comentarios son objetivos respecto a la subjetividad y respecto a la polaridad existe comentarios positivos y negativos.

3. Predomina una mayor cantidad de comentarios neutros, como una mínima cantidad de positivos. Pero, mayor respecto a los comentarios negativos.

$\large\text{10. Adicional: Nube de tags (palabras más significativas)}$
"""

from wordcloud import WordCloud

text = ' '.join(df['Comment'])
text

wordcloud = WordCloud(width=1024, height=800, colormap="Reds", min_font_size=14).generate(text)

plt.figure(figsize=(12, 10), facecolor=None)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()